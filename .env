# --- LLM Configuration for Akashvani ---
#
# LLM_MODEL: Specifies the name of the Ollama model to use.
#            Ensure this model is pulled and available in your Ollama instance.
#            Example: llama2, mistral, gemma:2b
LLM_MODEL=llama3

# LLM_HOST: The hostname or IP address where your Ollama server is running.
#           For a local Ollama instance, 'localhost' is typical.
LLM_HOST=localhost

# LLM_PORT: The port number on which your Ollama API is exposed.
#           The default port for Ollama is 11434.
LLM_PORT=11434

# HOST_OLLAMA_HOME: This variable is for reference and informational purposes only.
#                   It indicates the path to your Ollama home directory on the host machine.
#                   It is NOT used by the application for API calls but can be useful
#                   for debugging or setup instructions.
#                   Example: /Users/youruser/.ollama or C:\Users\youruser\.ollama
HOST_OLLAMA_HOME=/path/to/your/ollama_home/

# --- Akashvani AI Persona & Behavior Configuration ---
#
# AKASHVANI_USERNAME: The display name for your AI participant in the chat.
#                     This is how messages from the AI will appear.
AKASHVANI_USERNAME=Akashvani

# AKASHVANI_SHORTHAND: A shorter alias or shorthand for addressing Akashvani in the chat.
#                      Users can use this prefix (e.g., @av) to explicitly ask Akashvani questions.
#                      Should start with '@'.
AKASHVANI_SHORTHAND=@av

# LLM_TEMPERATURE: Controls the randomness of the LLM's responses.
#                  Lower values (e.g., 0.1-0.5) make the output more focused and deterministic.
#                  Higher values (e.g., 0.7-1.0) make it more creative and varied.
#                  Must be a float between 0.0 and 1.0.
LLM_TEMPERATURE=0.5

# LLM_MAX_TOKENS: The maximum number of tokens (words/sub-words) Akashvani's response can contain.
#                 Adjust to control response length. A higher value allows for more detailed answers
#                 or longer summaries.
LLM_MAX_TOKENS=150

# LLM_CONTEXT_HISTORY_SIZE: The number of most recent chat messages (excluding Akashvani's own)
#                           to send as context to the LLM.
#                           A larger number provides more conversational history for the AI.
LLM_CONTEXT_HISTORY_SIZE=10
